# ==============================================================================
# Merged Lakeflow Source: appsflyer
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator
import io
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import csv
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/appsflyer/appsflyer.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the AppsFlyer connector with connection-level options.

            Expected options:
                - api_token: API token for AppsFlyer authentication (Bearer token).
                - base_url (optional): Override for AppsFlyer API base URL.
                  Defaults to https://hq1.appsflyer.com (US region).
                  Use https://eu-west1.appsflyer.com for EU region.
            """
            api_token = options.get("api_token")
            if not api_token:
                raise ValueError("AppsFlyer connector requires 'api_token' in options")

            self.base_url = options.get("base_url", "https://hq1.appsflyer.com").rstrip("/")
            self.api_token = api_token

            # Configure a session with proper headers for AppsFlyer API
            # Note: Management API (/api/mng/*) and Raw Data Export API (/export/*)
            # may require different headers
            self._session = requests.Session()
            self._session.headers.update(
                {
                    "Authorization": f"Bearer {api_token}",
                    "User-Agent": "lakeflow-appsflyer-connector/1.0",
                    "Accept": "application/json",
                }
            )

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.
            """
            return [
                "apps",
                "installs_report",
                "in_app_events_report",
                "uninstall_events_report",
                "organic_installs_report",
                "organic_in_app_events_report",
            ]

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            The schema is static and derived from the AppsFlyer API documentation
            and connector design.
            """
            schema_map = {
                "apps": self._get_apps_schema,
                "installs_report": self._get_installs_report_schema,
                "in_app_events_report": self._get_in_app_events_report_schema,
                "uninstall_events_report": self._get_uninstall_events_report_schema,
                # Same as installs
                "organic_installs_report": self._get_installs_report_schema,
                # Same as in_app_events
                "organic_in_app_events_report": self._get_in_app_events_report_schema,
            }

            if table_name not in schema_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return schema_map[table_name]()

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.
            """
            metadata_map = {
                "apps": {
                    "primary_keys": ["app_id"],
                    "ingestion_type": "snapshot",
                },
                "installs_report": {
                    "primary_keys": ["appsflyer_id", "event_time"],
                    "cursor_field": "event_time",
                    "ingestion_type": "cdc",
                },
                "in_app_events_report": {
                    "primary_keys": ["appsflyer_id", "event_time", "event_name"],
                    "cursor_field": "event_time",
                    "ingestion_type": "cdc",
                },
                "uninstall_events_report": {
                    "primary_keys": ["appsflyer_id", "event_time"],
                    "cursor_field": "event_time",
                    "ingestion_type": "cdc",
                },
                "organic_installs_report": {
                    "primary_keys": ["appsflyer_id", "event_time"],
                    "cursor_field": "event_time",
                    "ingestion_type": "cdc",
                },
                "organic_in_app_events_report": {
                    "primary_keys": ["appsflyer_id", "event_time", "event_name"],
                    "cursor_field": "event_time",
                    "ingestion_type": "cdc",
                },
            }

            if table_name not in metadata_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return metadata_map[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table and return raw JSON-like dictionaries.

            For event reports (installs, in_app_events, etc.), this method:
                - Uses `/export/<app_id>/<report_type>/v5` endpoint
                - Supports incremental reads via date range filtering
                - Returns records with event_time as the cursor

            Required table_options for event reports:
                - app_id: AppsFlyer application identifier

            Optional table_options:
                - start_date: Initial ISO date (YYYY-MM-DD) for first run if no start_offset
                - lookback_hours: Lookback window in hours (default: 6)
                - max_days_per_batch: Maximum days to fetch per batch (default: 7)
            """
            reader_map = {
                "apps": self._read_apps,
                "installs_report": self._read_installs_report,
                "in_app_events_report": self._read_in_app_events_report,
                "uninstall_events_report": self._read_uninstall_events_report,
                "organic_installs_report": self._read_organic_installs_report,
                "organic_in_app_events_report": self._read_organic_in_app_events_report,
            }

            if table_name not in reader_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return reader_map[table_name](start_offset, table_options)

        def _get_apps_schema(self) -> StructType:
            """Return the apps table schema."""
            return StructType(
                [
                    StructField("app_id", StringType(), False),
                    StructField("app_name", StringType(), True),
                    StructField("platform", StringType(), True),
                    StructField("bundle_id", StringType(), True),
                    StructField("time_zone", StringType(), True),
                    StructField("currency", StringType(), True),
                    StructField("status", StringType(), True),
                ]
            )

        def _get_installs_report_schema(self) -> StructType:
            """Return the installs_report table schema."""
            return StructType(
                [
                    # Event identification
                    StructField("attributed_touch_type", StringType(), True),
                    StructField("attributed_touch_time", StringType(), True),
                    StructField("install_time", StringType(), True),
                    StructField("event_time", StringType(), False),
                    StructField("event_name", StringType(), True),
                    StructField("event_value", StringType(), True),
                    StructField("event_revenue", DoubleType(), True),
                    StructField("event_revenue_currency", StringType(), True),
                    StructField("event_revenue_usd", DoubleType(), True),
                    StructField("event_source", StringType(), True),
                    StructField("is_receipt_validated", BooleanType(), True),
                    # Attribution fields
                    StructField("af_prt", StringType(), True),
                    StructField("media_source", StringType(), True),
                    StructField("channel", StringType(), True),
                    StructField("keywords", StringType(), True),
                    StructField("campaign", StringType(), True),
                    StructField("campaign_id", StringType(), True),
                    StructField("adset", StringType(), True),
                    StructField("adset_id", StringType(), True),
                    StructField("ad", StringType(), True),
                    StructField("ad_id", StringType(), True),
                    StructField("ad_type", StringType(), True),
                    StructField("site_id", StringType(), True),
                    StructField("sub_site_id", StringType(), True),
                    StructField("sub_param_1", StringType(), True),
                    StructField("sub_param_2", StringType(), True),
                    StructField("sub_param_3", StringType(), True),
                    StructField("sub_param_4", StringType(), True),
                    StructField("sub_param_5", StringType(), True),
                    # Cost fields
                    StructField("cost_model", StringType(), True),
                    StructField("cost_value", DoubleType(), True),
                    StructField("cost_currency", StringType(), True),
                    # Multi-touch attribution contributors
                    StructField("contributor_1_af_prt", StringType(), True),
                    StructField("contributor_1_media_source", StringType(), True),
                    StructField("contributor_1_campaign", StringType(), True),
                    StructField("contributor_1_touch_type", StringType(), True),
                    StructField("contributor_1_touch_time", StringType(), True),
                    StructField("contributor_2_af_prt", StringType(), True),
                    StructField("contributor_2_media_source", StringType(), True),
                    StructField("contributor_2_campaign", StringType(), True),
                    StructField("contributor_2_touch_type", StringType(), True),
                    StructField("contributor_2_touch_time", StringType(), True),
                    StructField("contributor_3_af_prt", StringType(), True),
                    StructField("contributor_3_media_source", StringType(), True),
                    StructField("contributor_3_campaign", StringType(), True),
                    StructField("contributor_3_touch_type", StringType(), True),
                    StructField("contributor_3_touch_time", StringType(), True),
                    # Geographic fields
                    StructField("region", StringType(), True),
                    StructField("country_code", StringType(), True),
                    StructField("state", StringType(), True),
                    StructField("city", StringType(), True),
                    StructField("postal_code", StringType(), True),
                    StructField("dma", StringType(), True),
                    StructField("ip", StringType(), True),
                    # Network fields
                    StructField("wifi", BooleanType(), True),
                    StructField("operator", StringType(), True),
                    StructField("carrier", StringType(), True),
                    StructField("language", StringType(), True),
                    # Device identifiers
                    StructField("appsflyer_id", StringType(), False),
                    StructField("advertising_id", StringType(), True),
                    StructField("idfa", StringType(), True),
                    StructField("android_id", StringType(), True),
                    StructField("customer_user_id", StringType(), True),
                    StructField("imei", StringType(), True),
                    StructField("idfv", StringType(), True),
                    # Device info
                    StructField("platform", StringType(), True),
                    StructField("device_type", StringType(), True),
                    StructField("device_model", StringType(), True),
                    StructField("device_category", StringType(), True),
                    StructField("os_version", StringType(), True),
                    # App info
                    StructField("app_version", StringType(), True),
                    StructField("sdk_version", StringType(), True),
                    StructField("app_id", StringType(), True),
                    StructField("app_name", StringType(), True),
                    StructField("bundle_id", StringType(), True),
                    # Retargeting fields
                    StructField("is_retargeting", BooleanType(), True),
                    StructField("retargeting_conversion_type", StringType(), True),
                    # Additional attribution fields
                    StructField("af_siteid", StringType(), True),
                    StructField("match_type", StringType(), True),
                    StructField("attribution_lookback", StringType(), True),
                    StructField("af_keywords", StringType(), True),
                    StructField("http_referrer", StringType(), True),
                    StructField("original_url", StringType(), True),
                    StructField("user_agent", StringType(), True),
                    StructField("is_primary_attribution", BooleanType(), True),
                ]
            )

        def _get_in_app_events_report_schema(self) -> StructType:
            """Return the in_app_events_report table schema."""
            # Inherits most fields from installs schema, with additional event fields
            return StructType(
                [
                    # Event-specific fields
                    StructField("event_time", StringType(), False),
                    StructField("event_name", StringType(), False),
                    StructField("event_value", StringType(), True),
                    StructField("event_revenue", DoubleType(), True),
                    StructField("event_revenue_currency", StringType(), True),
                    StructField("event_revenue_usd", DoubleType(), True),
                    StructField("af_revenue", DoubleType(), True),
                    StructField("af_currency", StringType(), True),
                    StructField("af_quantity", LongType(), True),
                    StructField("af_content_id", StringType(), True),
                    StructField("af_content_type", StringType(), True),
                    StructField("af_price", DoubleType(), True),
                    # Identity fields
                    StructField("appsflyer_id", StringType(), False),
                    StructField("customer_user_id", StringType(), True),
                    StructField("install_time", StringType(), True),
                    # Attribution (inherited from install)
                    StructField("media_source", StringType(), True),
                    StructField("campaign", StringType(), True),
                    StructField("adset", StringType(), True),
                    StructField("ad", StringType(), True),
                    StructField("channel", StringType(), True),
                    StructField("keywords", StringType(), True),
                    # Device info
                    StructField("platform", StringType(), True),
                    StructField("device_model", StringType(), True),
                    StructField("os_version", StringType(), True),
                    # App info
                    StructField("app_id", StringType(), True),
                    StructField("app_version", StringType(), True),
                    StructField("sdk_version", StringType(), True),
                    # Geographic
                    StructField("country_code", StringType(), True),
                    StructField("city", StringType(), True),
                    StructField("region", StringType(), True),
                ]
            )

        def _get_uninstall_events_report_schema(self) -> StructType:
            """Return the uninstall_events_report table schema."""
            return StructType(
                [
                    StructField("event_time", StringType(), False),
                    StructField("event_name", StringType(), True),
                    StructField("appsflyer_id", StringType(), False),
                    StructField("customer_user_id", StringType(), True),
                    StructField("install_time", StringType(), True),
                    StructField("media_source", StringType(), True),
                    StructField("campaign", StringType(), True),
                    StructField("platform", StringType(), True),
                    StructField("app_id", StringType(), True),
                    StructField("country_code", StringType(), True),
                ]
            )

        def _read_apps(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the apps snapshot table.
            """
            url = f"{self.base_url}/api/mng/apps"
            response = self._session.get(url, timeout=60)

            if response.status_code != 200:
                raise RuntimeError(
                    f"AppsFlyer API error for apps: {response.status_code} {response.text}"
                )

            data = response.json()

            # Handle different response formats
            if isinstance(data, list):
                apps = data
            elif isinstance(data, dict):
                # Response might be wrapped in a dict with 'apps' or 'data' key
                apps = data.get('apps') or data.get('data') or [data]
            else:
                raise ValueError(
                    f"Unexpected response format for apps: {type(data).__name__}"
                )

            # Transform JSON API format to flat structure
            # API returns: {"id": "...", "type": "app", "attributes": {...}}
            # We need: {"app_id": "...", "app_name": "...", ...}
            def flatten_app(app):
                if isinstance(app, dict) and 'attributes' in app:
                    # JSON API format
                    flattened = {
                        'app_id': app.get('id'),
                        'app_name': app.get('attributes', {}).get('name'),
                        'platform': app.get('attributes', {}).get('platform'),
                        'bundle_id': app.get('id'),  # Use id as bundle_id
                        'time_zone': app.get('attributes', {}).get('time_zone'),
                        'currency': app.get('attributes', {}).get('currency'),
                        'status': app.get('attributes', {}).get('status'),
                    }
                    return flattened
                # Already flat format
                return app

            flattened_apps = (flatten_app(app) for app in apps)
            return flattened_apps, {}

        def _read_installs_report(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the installs_report table using the raw data export API.
            """
            return self._read_event_report(
                "installs_report", start_offset, table_options
            )

        def _read_in_app_events_report(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the in_app_events_report table.
            """
            return self._read_event_report(
                "in_app_events_report", start_offset, table_options
            )

        def _read_uninstall_events_report(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the uninstall_events_report table.
            """
            return self._read_event_report(
                "uninstall_events_report", start_offset, table_options
            )

        def _read_organic_installs_report(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the organic_installs_report table.
            """
            return self._read_event_report(
                "organic_installs_report", start_offset, table_options
            )

        def _read_organic_in_app_events_report(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the organic_in_app_events_report table.
            """
            return self._read_event_report(
                "organic_in_app_events_report", start_offset, table_options
            )

        # pylint: disable=too-many-locals,too-many-branches,too-many-statements
        def _read_event_report(
            self,
            report_type: str,
            start_offset: dict,
            table_options: dict[str, str],
            is_aggregated: bool = False,
        ) -> (Iterator[dict], dict):
            """
            Internal implementation for reading event-based reports.

            Args:
                report_type: The report endpoint name (e.g., 'installs_report')
                start_offset: Dictionary containing cursor information
                table_options: Additional options including app_id, start_date, etc.
                is_aggregated: Whether this is an aggregated report (uses 'date' cursor)
            """
            app_id = table_options.get("app_id")
            if not app_id:
                raise ValueError(
                    f"table_options for '{report_type}' must include non-empty 'app_id'"
                )

            # Get lookback hours (default 6 hours for late-arriving events)
            try:
                lookback_hours = int(table_options.get("lookback_hours", 6))
            except (TypeError, ValueError):
                lookback_hours = 6

            # Get max days per batch (default 7 days)
            try:
                max_days_per_batch = int(table_options.get("max_days_per_batch", 7))
            except (TypeError, ValueError):
                max_days_per_batch = 7

            # Determine the starting cursor
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            # Calculate date range
            if cursor:
                # Parse cursor and apply lookback
                try:
                    cursor_dt = datetime.strptime(cursor, "%Y-%m-%d")
                    from_dt = cursor_dt - timedelta(hours=lookback_hours)
                except (ValueError, TypeError):
                    # If cursor is not a valid date, default to 30 days ago
                    from_dt = datetime.utcnow() - timedelta(days=30)
            else:
                # No cursor, default to 30 days ago
                from_dt = datetime.utcnow() - timedelta(days=30)

            to_dt = datetime.utcnow()

            # Limit to max_days_per_batch
            if (to_dt - from_dt).days > max_days_per_batch:
                to_dt = from_dt + timedelta(days=max_days_per_batch)

            from_date = from_dt.strftime("%Y-%m-%d")
            to_date = to_dt.strftime("%Y-%m-%d")

            # Build request URL
            # Raw Data Export API path: /api/raw-data/export/app/{app_id}/{report_type}/v5
            url = f"{self.base_url}/api/raw-data/export/app/{app_id}/{report_type}/v5"
            params = {
                "from": from_date,
                "to": to_date,
                "timezone": "UTC",
            }

            # Make API request with CSV Accept header
            # Note: Raw Data Export API expects CSV format
            headers = {"Accept": "text/csv"}
            response = self._session.get(url, params=params, headers=headers, timeout=120)

            if response.status_code != 200:
                raise RuntimeError(
                    f"AppsFlyer API error for {report_type}: {response.status_code} {response.text}"
                )

            # Parse CSV response
            # Raw Data Export API returns CSV with UTF-8 BOM
            text = response.content.decode('utf-8-sig')
            if not text or not text.strip():
                # Empty response, return empty iterator
                print(f"[AppsFlyer] No data for {report_type}: {from_date} to {to_date}")
                return iter([]), cursor

            # Parse CSV into list of dictionaries
            csv_reader = csv.DictReader(io.StringIO(text))
            raw_data = list(csv_reader)
            print(f"[AppsFlyer] {report_type}: Parsed {len(raw_data)} raw records from CSV")

            # Normalize field names and values
            # CSV headers are like "Event Time" but schema expects "event_time"
            # Empty strings should be None for proper type conversion
            def normalize_record(record):
                return {
                    key.lower().replace(' ', '_'): (value if value != '' else None)
                    for key, value in record.items()
                }

            data = [normalize_record(record) for record in raw_data]
            print(
                f"[AppsFlyer] {report_type}: Normalized {len(data)} records, "
                f"date range: {from_date} to {to_date}"
            )

            # Find the maximum event_time or date for the next cursor
            max_cursor = cursor
            cursor_field = "date" if is_aggregated else "event_time"

            for record in data:
                record_time = record.get(cursor_field)
                if isinstance(record_time, str):
                    if max_cursor is None or record_time > max_cursor:
                        max_cursor = record_time

            # Compute next offset
            if max_cursor:
                # For event_time (ISO datetime), extract just the date part
                if not is_aggregated and max_cursor:
                    try:
                        max_cursor = max_cursor[:10]  # Extract YYYY-MM-DD
                    except:
                        pass

            # Return records and next offset
            next_offset = {"cursor": max_cursor} if max_cursor else {}
            return iter(data), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
