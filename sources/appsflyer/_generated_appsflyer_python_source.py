# ==============================================================================
# Merged Lakeflow Source: appsflyer
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)
import io
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from urllib.parse import quote
from pyspark.sql.types import *
import base64
import csv
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/appsflyer/appsflyer.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: Dict[str, str]) -> None:
            """
            Initialize the AppsFlyer connector with authentication and configuration.

            Required options:
            - api_token: AppsFlyer API V2 token
            - app_id: Application identifier (iOS bundle ID or Android package name)

            Optional options:
            - start_date: Initial sync start date (default: 90 days ago)
            - lookback_days: Lookback window for incremental sync (default: 3)
            """
            self.api_token = options["api_token"]
            self.app_id = options["app_id"]
            self.start_date = options.get("start_date", None)
            self.lookback_days = int(options.get("lookback_days", "3"))
            self.base_url = f"https://hq1.appsflyer.com/api/raw-data/export/app/{self.app_id}"
            self.auth_header = {"Authorization": f"Bearer {self.api_token}"}

            # Maximum rows per request (AppsFlyer supports up to 1M)
            self.max_rows = 1000000

        def list_tables(self) -> List[str]:
            """
            Returns a list of available AppsFlyer report types.
            Note: Currently only organic_in_app_events_report is enabled for testing.
            Other reports are commented out due to subscription limitations or unavailability.
            """
            return [
                # Ad Engagement Reports
                # "clicks_report",  # Requires higher subscription tier
                # "impressions_report",  # Requires higher subscription tier
                # User Acquisition Reports (Non-Organic)
                # "installs_report",  # Requires higher subscription tier
                # "in_app_events_report",  # Requires higher subscription tier
                # "sessions_report",  # Requires higher subscription tier
                # "uninstall_events_report",  # Requires higher subscription tier
                # "attributed_ad_revenue_report",  # Requires higher subscription tier
                # User Acquisition Reports (Organic)
                # "organic_installs_report",  # Requires higher subscription tier
                "organic_in_app_events_report",
                # "organic_sessions_report",  # Requires higher subscription tier
                # "organic_uninstall_events_report",  # Requires higher subscription tier
                # "organic_ad_revenue_report",  # Requires higher subscription tier
                # Retargeting Reports
                # "conversions_report",  # Requires higher subscription tier
                # "retargeting_in_app_events_report",  # Requires higher subscription tier
                # "retargeting_sessions_report",  # Requires higher subscription tier
                # "retargeting_ad_revenue_report",  # Requires higher subscription tier
                # Postback Reports
                # "install_postbacks_report",  # Requires higher subscription tier
                # "inapps_postbacks_report",  # Requires higher subscription tier
                # "conversions_postbacks_report",  # Requires higher subscription tier
                # "retargeting_inapps_postbacks_report",  # Requires higher subscription tier
                # Fraud & Protection Reports
                # "blocked_installs_report",  # Requires Protect360 subscription
                # "blocked_install_postbacks_report",  # Requires Protect360 subscription
                # "blocked_clicks_report",  # Requires higher subscription tier
                # "post_attribution_installs_report",  # Unknown report
                # "blocked_inapps_events_report",  # Unknown report
            ]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Returns the schema for the specified AppsFlyer report.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Table '{table_name}' is not supported.")

            # Base schema fields present in all reports
            base_fields = [
                StructField("appsflyer_id", StringType(), True),
                StructField("event_time", TimestampType(), True),
                StructField("install_time", TimestampType(), True),
                StructField("event_name", StringType(), True),
                StructField("event_type", StringType(), True),
                StructField("event_value", StringType(), True),
                StructField("event_revenue", DoubleType(), True),
                StructField("event_revenue_currency", StringType(), True),
                StructField("event_revenue_usd", DoubleType(), True),
            ]

            # Attribution fields (null in organic reports)
            attribution_fields = [
                StructField("attributed_touch_type", StringType(), True),
                StructField("attributed_touch_time", TimestampType(), True),
                StructField("media_source", StringType(), True),
                StructField("campaign", StringType(), True),
                StructField("af_channel", StringType(), True),
                StructField("af_ad", StringType(), True),
                StructField("af_ad_id", StringType(), True),
                StructField("af_adset", StringType(), True),
                StructField("af_c_id", StringType(), True),
                StructField("match_type", StringType(), True),
                StructField("af_keywords", StringType(), True),
                StructField("af_cost_value", DoubleType(), True),
                StructField("af_cost_currency", StringType(), True),
            ]

            # Device fields
            device_fields = [
                StructField("advertising_id", StringType(), True),
                StructField("idfa", StringType(), True),
                StructField("idfv", StringType(), True),
                StructField("android_id", StringType(), True),
                StructField("imei", StringType(), True),
                StructField("device_model", StringType(), True),
                StructField("os_version", StringType(), True),
                StructField("platform", StringType(), True),
                StructField("language", StringType(), True),
            ]

            # Location fields
            location_fields = [
                StructField("country_code", StringType(), True),
                StructField("city", StringType(), True),
                StructField("region", StringType(), True),
                StructField("postal_code", StringType(), True),
                StructField("ip", StringType(), True),
            ]

            # App fields
            app_fields = [
                StructField("app_id", StringType(), True),
                StructField("app_name", StringType(), True),
                StructField("app_version", StringType(), True),
                StructField("bundle_id", StringType(), True),
            ]

            # Network fields
            network_fields = [
                StructField("carrier", StringType(), True),
                StructField("wifi", BooleanType(), True),
                StructField("user_agent", StringType(), True),
            ]

            # IAP & Subscription fields
            iap_fields = [
                StructField("af_product_id", StringType(), True),
                StructField("af_purchase_date_ms", LongType(), True),
                StructField("af_transaction_id", StringType(), True),
                StructField("af_order_id", StringType(), True),
                StructField("af_net_revenue", DoubleType(), True),
                StructField("af_store", StringType(), True),
                StructField("af_currency", StringType(), True),
                StructField("af_price", DoubleType(), True),
                StructField("af_quantity", LongType(), True),
            ]

            # Ad Revenue fields
            ad_revenue_fields = [
                StructField("ad_revenue_ad_type", StringType(), True),
                StructField("mediation_network", StringType(), True),
                StructField("placement", StringType(), True),
                StructField("impressions", LongType(), True),
            ]

            # Fraud Prevention fields
            fraud_fields = [
                StructField("blocked_reason", StringType(), True),
                StructField("is_organic", StringType(), True),
                StructField("rejected_reason", StringType(), True),
            ]

            # Combine all fields
            all_fields = (
                base_fields +
                attribution_fields +
                device_fields +
                location_fields +
                app_fields +
                network_fields +
                iap_fields +
                ad_revenue_fields +
                fraud_fields
            )

            return StructType(all_fields)

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict:
            """
            Returns metadata for the specified AppsFlyer report.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Table '{table_name}' is not supported.")

            # Determine primary keys based on report type
            if "installs_report" in table_name or "uninstall" in table_name:
                primary_keys = ["appsflyer_id"]
            elif "events_report" in table_name or "sessions_report" in table_name:
                primary_keys = ["appsflyer_id", "event_time", "event_name"]
            elif "conversions" in table_name or "clicks" in table_name or "impressions" in table_name:
                primary_keys = ["appsflyer_id", "event_time"]
            elif "postbacks" in table_name:
                primary_keys = ["appsflyer_id", "event_time"]
            elif "blocked" in table_name or "post_attribution" in table_name:
                primary_keys = ["appsflyer_id", "event_time"]
            else:
                primary_keys = ["appsflyer_id"]

            # Determine ingestion type
            if "uninstall" in table_name:
                ingestion_type = "cdc"
            else:
                ingestion_type = "append"

            metadata = {
                "primary_keys": primary_keys,
                "cursor_field": "event_time",
                "ingestion_type": ingestion_type,
            }

            return metadata

        def _get_date_range(self, start_offset: dict) -> tuple:
            """Determine the date range for data extraction."""
            if start_offset and "from_date" in start_offset and "to_date" in start_offset:
                from_date = datetime.fromisoformat(start_offset["from_date"])
                to_date = datetime.fromisoformat(start_offset["to_date"])
            else:
                # Initial sync - use start_date or default to 90 days ago
                from_date = (
                    datetime.fromisoformat(self.start_date)
                    if self.start_date
                    else datetime.now() - timedelta(days=90)
                )
                # Read one day at a time initially
                to_date = from_date + timedelta(days=1)
            return from_date, to_date

        def _build_query_params(
            self, from_date: datetime, to_date: datetime, table_options: Dict[str, str]
        ) -> dict:
            """Build query parameters for API request."""
            params = {
                "from": from_date.strftime("%Y-%m-%d %H:%M:%S"),
                "to": to_date.strftime("%Y-%m-%d %H:%M:%S"),
                "maximum_rows": str(self.max_rows),
            }

            # Add optional table_options
            optional_params = [
                "event_name", "media_source", "geo",
                "timezone", "currency", "additional_fields"
            ]
            for param in optional_params:
                if param in table_options:
                    params[param] = table_options[param]

            return params

        def _calculate_next_offset(
            self, num_records: int, from_date: datetime, to_date: datetime
        ) -> dict:
            """Calculate the next offset for pagination."""
            current_time = datetime.now()

            if num_records >= self.max_rows:
                # Hit row limit - need to split time range
                time_diff = to_date - from_date
                if time_diff.total_seconds() <= 3600:  # Already at 1 hour or less
                    # Can't split further, move to next hour
                    return {
                        "from_date": to_date.isoformat(),
                        "to_date": min(to_date + timedelta(hours=1), current_time).isoformat(),
                    }
                # Split the range in half
                mid_date = from_date + (time_diff / 2)
                return {
                    "from_date": from_date.isoformat(),
                    "to_date": mid_date.isoformat(),
                }

            if to_date >= current_time:
                # Caught up to current time - return same offset to signal completion
                return {
                    "from_date": from_date.isoformat(),
                    "to_date": to_date.isoformat(),
                }

            # Move to next time window
            return {
                "from_date": to_date.isoformat(),
                "to_date": min(to_date + timedelta(days=1), current_time).isoformat(),
            }

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Reads data from the specified AppsFlyer report.

            Optional table_options:
            - event_name: Filter specific events (comma-separated)
            - media_source: Filter by media source
            - geo: Filter by country code
            - timezone: Timezone offset
            - currency: Currency preference (USD or preferred)
            - additional_fields: Additional fields to include (comma-separated)
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Table '{table_name}' is not supported.")

            # Determine date range for this read
            from_date, to_date = self._get_date_range(start_offset)

            # Build API request
            endpoint = f"{self.base_url}/{table_name}/v5"
            params = self._build_query_params(from_date, to_date, table_options)

            # Make API request
            response = requests.get(endpoint, headers=self.auth_header, params=params)

            if response.status_code != 200:
                raise Exception(
                    f"AppsFlyer API error for {table_name}: {response.status_code} {response.text}"
                )

            # Parse CSV response
            records_list = list(self._parse_csv_response(response.text))

            # Calculate next offset
            next_offset = self._calculate_next_offset(len(records_list), from_date, to_date)

            return iter(records_list), next_offset

        def _parse_timestamp(self, record: dict, field: str) -> None:
            """Parse timestamp field in-place."""
            if field in record and record[field]:
                try:
                    record[field] = datetime.strptime(
                        record[field], "%Y-%m-%d %H:%M:%S"
                    )
                except (ValueError, TypeError):
                    record[field] = None

        def _parse_numeric_fields(self, record: dict) -> None:
            """Parse all numeric fields in-place."""
            # Parse float fields
            float_fields = [
                "event_revenue", "event_revenue_usd", "af_cost_value",
                "af_net_revenue", "af_price"
            ]
            for field in float_fields:
                if field in record and record[field]:
                    try:
                        record[field] = float(record[field])
                    except (ValueError, TypeError):
                        record[field] = None

            # Parse integer fields
            integer_fields = [
                "af_purchase_date_ms", "af_quantity", "impressions"
            ]
            for field in integer_fields:
                if field in record and record[field]:
                    try:
                        record[field] = int(record[field])
                    except (ValueError, TypeError):
                        record[field] = None

            # Parse boolean fields
            if "wifi" in record and record["wifi"]:
                record["wifi"] = record["wifi"].lower() == "true"

        def _parse_csv_response(self, csv_text: str) -> List[dict]:
            """
            Parses CSV response from AppsFlyer API into list of dictionaries.
            """
            if not csv_text or csv_text.strip() == "":
                return []

            records = []
            csv_reader = csv.DictReader(io.StringIO(csv_text))

            for row in csv_reader:
                # Convert empty strings to None
                record = {k: (v if v != "" else None) for k, v in row.items()}

                # Parse timestamps
                for timestamp_field in ["event_time", "install_time", "attributed_touch_time"]:
                    self._parse_timestamp(record, timestamp_field)

                # Parse numeric fields
                self._parse_numeric_fields(record)

                records.append(record)

            return records


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
